# Behind the scenes - Investigator Grants 2019

This repository contains source code and test output for analysis of the NHMRC Fellowship scheme outcomes 2015 - 2019. This work supported the analysis presented in "[Investigating the Investigator Grants 2019](https://dezeraecox.com/2019/09/30/investigating-the-investigator-grants-2019/)", originally posted September 30th 2019.

## Raw data

The raw data used in this analysis came from three main sources.

The first was the [NHMRC grant outcomes website](https://www.nhmrc.gov.au/funding/data-research/outcomes-funding-rounds), which offers spreadsheet summaries for grants released since 2013. I chose to use only the data from 2015 onwards, for a couple of reasons: (1) the structure of the Fellowship system appears to have changed in 2014 to the ECF, CDF, RF layout which remained in place until 2018. This meant that 2013 data was poorly correlated with the more recent datasets. (2) The 2014 dataset did not have as much detail in the gender, age, state breakdowns that could be easily compared to the following years. (3) Five years seemed like a nice time period to work with!

The second source of data was the Field of Research codes used to classify research. You can find the complete list at the [Australian Bureau of Statistics](https://www.abs.gov.au/ausstats/abs@.nsf/0/6BB427AB9696C225CA2574180004463E?opendocument). I struggled to find an easily-downloadable version, and instead copied them from the University of Melbourne intranet. With a little post-processing, I had a fully functional list of each level of classification, which I could then use to understand which types of research were popular for funding.

The last source of data was [Scival](https://www.scival.com/home), which I used to gather the number of research publications and average Field-Weighted-Citation-Impact (FWCI) for each awardee in the ten years previous to their year of award. This was somewhat of a manual process, and I used the 'best match' profile for each awardee imported into SciVal. Overally, 88% of the awardees were matched accurately (and this could be increased with a little manual curation). I also did a little digging around in the PubMed Central API using a python package (see the resources list below for more information) to batch-query the author names and collect their publication history, to compare with the matches generated by SciVal. For more details on this process, keep an eye out for my Behind the Scenes post!



## Disclaimer

This analysis was intended to inform my _personal_ decision of whether to apply for an I Grant in the 2019 round. It is not intended as a complete, in-depth assessment of the outcomes. Therefore, the information contained here and in the original post is provided on an “as is” basis with no guarantees of completeness, accuracy, usefulness or timeliness. Any action you take as a result of this information is done so at your own peril. If you do decide to act on this information, I wish you the best of luck whichever path you may choose. May the odds be ever in your favour.

This being said, I have of course aimed to be as unbiased and informative as possible. This is also my first foray into data-science-for-public-consumption, so if you do notice any overt errors or bugs feel free to raise an issue and I will check it out as soon as possible.